{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7U/TRaX8jpOlKxvcJNRw/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swagathmullangi/learn_agentic_ai/blob/main/Langgraph_MultiAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EkwHWvwCoLxF"
      },
      "outputs": [],
      "source": [
        "# ! pip install langchain_community langchain_openai faiss-cpu langchain_tavily langchain sentence-transformers langchain-huggingface langchain-groq crewai crewai_tools langgraph langchain-google-genai grandalf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, Literal, List, TypedDict, Union\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers.string import StrOutputParser\n",
        "\n",
        "import operator"
      ],
      "metadata": {
        "id": "tknDjQRRoxYF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "    query: str\n",
        "    route: str\n",
        "    documents: List[str]\n",
        "    generation: str\n",
        "    response: str\n",
        "    messages: Annotated[List[BaseMessage], add_messages]"
      ],
      "metadata": {
        "id": "x76eKgZztv57"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most appropriate data source.\"\"\"\n",
        "    source: Union[\n",
        "        Annotated[str, \"vectorstore\", \"Use this to answer any questions related to transformers or attention\"],\n",
        "        Annotated[str, \"web_search\", \"Use this to answer anything not from pdf or most recent events and also if you want to search web\"],\n",
        "        Annotated[str, \"generation\", \"Use this if the query is a direct instruction to generate content and doesnot fit 'vectorstore' or 'web_search'.\"]\n",
        "    ] = Field(description=\"Given a user question, choose ONE of the following datasources: 'vectorstore', 'web_search', or 'generation'\")"
      ],
      "metadata": {
        "id": "8uuroL1RrebM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating llm object\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model = \"gemini-2.0-flash\", api_key=\"AIzaSyBfQuvlI0_D1erZk1nquxLZulbvsBat7fM\")"
      ],
      "metadata": {
        "id": "9qQnzVuWrsut"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def router_node(state: State):\n",
        "  structured_llm_router = llm.with_structured_output(RouteQuery)\n",
        "  query = state[\"query\"]\n",
        "  prompt_message = [HumanMessage(content=f\"\"\"Your role is to route the users query to the most appropriate data source based on RouteQuery\n",
        "  User Query: \"{query}\"\n",
        "  Based on the query and the descriptions, choose exactly one of the following data sources: 'vectorstore', 'web_search', or 'generation'.\n",
        "  \"\"\")\n",
        "  ]\n",
        "  result = structured_llm_router.invoke(prompt_message)\n",
        "  print(f'Routed to: {result.source}')\n",
        "  if result.source == \"vectorstore\":\n",
        "      return {\"route\": \"vectorstore\"}\n",
        "  elif result.source == \"web_search\":\n",
        "      return {\"route\": \"web_search\"}\n",
        "  else: # generation\n",
        "      return {\"route\": \"generation\"}"
      ],
      "metadata": {
        "id": "UjZguJy2r8ws"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "bwUKqGq_tlG8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file = \"/content/1706.03762v7.pdf\""
      ],
      "metadata": {
        "id": "wiTGmUd3vPpt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "EQAcLxEmx3Z2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pdf_and_build_vectorstore(pdf_file_path: str):\n",
        "    global pdf_vectorstore, embedding_model\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "    doc_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    pdf_vectorstore = FAISS.from_documents(doc_splits, embedding_model)\n",
        "\n",
        "process_pdf_and_build_vectorstore(pdf_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSKYnje9xr_m",
        "outputId": "84eba4ae-7f12-4288-d098-e244560cc34b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "web_search_tool = TavilySearchResults(max_results=1)\n",
        "\n",
        "def web_search_tool_node(state: State):\n",
        "    query = state[\"query\"]\n",
        "    docs = web_search_tool.invoke({\"query\": query})\n",
        "    retrieved_docs = [d[\"content\"] for d in docs]\n",
        "    return {\"documents\": retrieved_docs, \"generation\": \"\", \"route\": \"web_search\"} # Clear generation, update route\n",
        "\n",
        "def generation_tool_node(state: State):\n",
        "    query = state[\"query\"]\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful assistant. Directly answer the user's query or fulfill their instruction.\"),\n",
        "        (\"user\", f\"{query}\")\n",
        "    ])\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    generated_text = chain.invoke({\"query\": query})\n",
        "    return {\"documents\": [], \"generation\": generated_text, \"route\": \"generated\"} # Clear documents,\n",
        "\n",
        "def vectorstore_tool_node(state: State):\n",
        "    query = state[\"query\"]\n",
        "    retrieved_langchain_docs = pdf_vectorstore.similarity_search(query, k=3)\n",
        "    retrieved_docs = [doc.page_content for doc in retrieved_langchain_docs]\n",
        "    return {\"documents\": retrieved_docs, \"generation\": \"\", \"route\": \"vectorstore\"}"
      ],
      "metadata": {
        "id": "CkU857h1siQi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retriever_task_node(state: State):\n",
        "  query = state[\"query\"]\n",
        "  final_response = \"\"\n",
        "  if state.get(\"documents\") and any(d.strip() for d in state[\"documents\"] if isinstance(d, str)):\n",
        "    context = \"\\n\\n\".join(state[\"documents\"])\n",
        "  elif state.get(\"generation\"):\n",
        "    context = state[\"generation\"]\n",
        "  else:\n",
        "    context = \"I was unable to retrieve relevant information or generate a direct\"\n",
        "  system_prompt = \"\"\"\n",
        "            You are a helpful assistant. Answer the user's query based ONLY on the following retrieved context.\n",
        "            If the context is not sufficient or doesn't contain the answer, clearly state that the information is not found in the provided context.\n",
        "            Do not use any external knowledge. Be concise.\\n\\n\n",
        "            Context: {context}\"\n",
        "        \"\"\"\n",
        "  prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_prompt),\n",
        "                                                      (\"user\", \"{query}\")])\n",
        "  chain = prompt_template | llm | StrOutputParser()\n",
        "  final_response = chain.invoke({\"context\": context, \"query\": query})\n",
        "  return {\"response\": final_response}"
      ],
      "metadata": {
        "id": "73rXi4551-ad"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_edges(state: State):\n",
        "    if state[\"route\"] == \"vectorstore\":\n",
        "        return \"vectorstore_tool\"\n",
        "    elif state[\"route\"] == \"web_search\":\n",
        "        return \"web_search_tool\"\n",
        "    elif state[\"route\"] == \"generation\":\n",
        "        return \"generation_tool\"\n",
        "    return \"end\""
      ],
      "metadata": {
        "id": "_iv1BjmP0Kxf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END"
      ],
      "metadata": {
        "id": "xjE9ygEK0r0H"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Graph Object\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# add nodes\n",
        "graph_builder.add_node(\"router\", router_node)\n",
        "graph_builder.add_node(\"vectorstore_tool\", vectorstore_tool_node)\n",
        "graph_builder.add_node(\"web_search_tool\", web_search_tool_node)\n",
        "graph_builder.add_node(\"generation_tool\", generation_tool_node)\n",
        "graph_builder.add_node(\"retriever_task\", retriever_task_node)\n",
        "\n",
        "# add edges\n",
        "graph_builder.add_edge(START, \"router\")\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"router\",\n",
        "    conditional_edges,\n",
        "    {\n",
        "        \"vectorstore_tool\": \"vectorstore_tool\",\n",
        "        \"web_search_tool\": \"web_search_tool\",\n",
        "        \"generation_tool\": \"generation_tool\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_builder.add_edge(\"vectorstore_tool\", \"retriever_task\")\n",
        "graph_builder.add_edge(\"web_search_tool\", \"retriever_task\")\n",
        "graph_builder.add_edge(\"generation_tool\", \"retriever_task\")\n",
        "graph_builder.add_edge(\"retriever_task\", END)\n",
        "\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "RdAWi64d03MA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(graph.get_graph().draw_ascii())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oOjTsWn1z-1",
        "outputId": "74d547dc-5d30-4939-cc22-75b0bbf6099c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                +-----------+                                  \n",
            "                                | __start__ |                                  \n",
            "                                +-----------+                                  \n",
            "                                       *                                       \n",
            "                                       *                                       \n",
            "                                       *                                       \n",
            "                                  +--------+                                   \n",
            "                                ..| router |...                                \n",
            "                           .....  +--------+   .....                           \n",
            "                      .....            .            .....                      \n",
            "                 .....                 .                 .....                 \n",
            "              ...                      .                      ...              \n",
            "+-----------------+          +------------------+          +-----------------+ \n",
            "| generation_tool |          | vectorstore_tool |          | web_search_tool | \n",
            "+-----------------+***       +------------------+        **+-----------------+ \n",
            "                      *****            *            *****                      \n",
            "                           *****       *       *****                           \n",
            "                                ***    *    ***                                \n",
            "                              +----------------+                               \n",
            "                              | retriever_task |                               \n",
            "                              +----------------+                               \n",
            "                                       *                                       \n",
            "                                       *                                       \n",
            "                                       *                                       \n",
            "                                  +---------+                                  \n",
            "                                  | __end__ |                                  \n",
            "                                  +---------+                                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"who is the current president of US?\"\n",
        "graph.invoke({\"query\":user_query})['response']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "PxAxzg0N4In1",
        "outputId": "4eff0a85-79a4-4c7b-93ee-8e66cca031f9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Routed to: web_search\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The 47th and current president of the United States is Donald John Trump.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what is the llm?\"\n",
        "graph.invoke({\"query\":user_query})['response']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "QLvxLJ7t5Zi2",
        "outputId": "5343b34c-9562-4aa1-e180-1449b8e6a912"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Routed to: generation\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"LLM stands for Large Language Model. It's a type of artificial intelligence (AI) model that is trained on a massive amount of text data to understand, summarize, generate, and predict new text.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"explain transformer architecture mentioned in the paper?\"\n",
        "graph.invoke({\"query\":user_query})['response']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "0ePpYoRHRix9",
        "outputId": "1e9d866e-f175-4598-d6af-5f1c1fba19e6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Routed to: vectorstore\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Transformer architecture uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism, and a simple, position-wise fully connected feed-forward network. A residual connection is employed around each of the two sub-layers, followed by layer normalization. All sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dafOYdgbZuk0"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}